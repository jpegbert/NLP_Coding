{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python实现n-gram\n",
    "\n",
    "本例子主要受 Michael Collins 教授的 Language Modeling 启发而编写，为了帮助大家理解语言模型，我在我的博客、公众号上发表了文章[一文读懂NLP中的语言模型(公众号)](http://mp.weixin.qq.com/s?__biz=MzIwNDM1NjUzMA==&mid=2247483658&idx=1&sn=9c5e7cc50b65cf31a08f1e2a0046ceb1&chksm=96c02fd7a1b7a6c1bbabe19145665d370020f4a3e89ebdc1226a1ec4ed110ef089c6fb0212c4&mpshare=1&scene=1&srcid=1114A1PGK4rDqKMMbsAmplr3#rd)，欢迎大家阅读。当然强烈推荐[Michael Collins 教授的 Language Modeling 原文](http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "1. [项目结构](#项目结构)\n",
    "\n",
    "2. [环境要求](#环境要求)\n",
    "\n",
    "3. [代码分析](#代码分析)\n",
    "\n",
    "4. [结果分析](#结果分析)\n",
    "\n",
    "5. [项目后续](#项目后续)\n",
    "\n",
    "6. [联系作者](#联系作者)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目结构\n",
    "\n",
    "| - src\n",
    "    \n",
    "    | - const.py      常量定义文件\n",
    "    \n",
    "    | - corpus        语料库\n",
    "    \n",
    "    | - dataset.py    加载语料\n",
    "    \n",
    "    | - evaluate.py   模型的评估方法\n",
    "    \n",
    "    | - main.py       例子程序\n",
    "    \n",
    "    | - ngram.py      ungram, bigram, trigram 模型，以及一些模型方法\n",
    "    \n",
    "    | - processing.py 字典的生成等处理方法\n",
    "    \n",
    "    | - smooth.py     平滑方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境要求\n",
    "\n",
    "    python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### const.py\n",
    "\n",
    "在这里定义了三个常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 未登录词\n",
    "UNK = None\n",
    "# 句子开始标记，代表句子的开头\n",
    "START_TOKEN = '<s>'\n",
    "# 句子结束标记，代表句子的结尾\n",
    "END_TOKEN = '</s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import const\n",
    "\n",
    "#加入起始标记\n",
    "def build_sentences(sentences):\n",
    "        out = []\n",
    "        for sentence in sentences:\n",
    "                words = [x.lower() for x in sentence]\n",
    "                words.insert(0, \"<s>\")\n",
    "                words.append(\"</s>\")\n",
    "                out.append(words)\n",
    "        return out\n",
    "\n",
    "# 构建ungram词频词典\n",
    "def build_undict(sentences):\n",
    "        undict = {}\n",
    "        total = 0\n",
    "        for words in sentences:\n",
    "                for word in words:\n",
    "                        if word not in undict:\n",
    "                                undict[word] = 1\n",
    "                        else:\n",
    "                                undict[word] += 1\n",
    "                        if word != const.START_TOKEN and word != const.END_TOKEN:\n",
    "                                total += 1\n",
    "        return undict, total\n",
    "\n",
    "# 构建bigram词频词典，其中以三元组(u, v)作为词典的键\n",
    "def build_bidict(sentences):\n",
    "    bidict = {}\n",
    "    for words in sentences:\n",
    "            for i in range(len(words)-1):\n",
    "                    tup = (words[i], words[i+1])\n",
    "                    if tup not in bidict:\n",
    "                            bidict[tup] = 1\n",
    "                    else:\n",
    "                            bidict[tup] += 1\n",
    "    return bidict\n",
    "\n",
    "# 构建trigram词频词典，其中以三元组(u, v, w)作为词典的键\n",
    "def build_tridict(sentences):\n",
    "        tridict = {}\n",
    "        for words in sentences:\n",
    "                for i in range(len(words) -2):\n",
    "                        tup = (words[i], words[i+1], words[i+2])\n",
    "                        if tup not in tridict:\n",
    "                                tridict[tup] = 1\n",
    "                        else:\n",
    "                                tridict[tup] += 1\n",
    "        return tridict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngram.py\n",
    "\n",
    "n-gram模型，实现了ungram, bigram, trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import const\n",
    "from processing import *\n",
    "\n",
    "'''\n",
    "@function calc_prob \t\t\t计算条件概率，这里使用最大似然估计(max-likelihood estimate)去计算概率\n",
    "@function calc_sentence_prob\t计算句子的条件概率\n",
    "'''\n",
    "class UnGram(object):\n",
    "\tdef __init__(self, sentences, smooth = None):\n",
    "\t\tself.undict, self.total = build_undict(sentences)\n",
    "\t\tself.smooth = smooth\n",
    "\n",
    "\tdef calc_prob(self, word):\n",
    "\t\tprob = 0\n",
    "\t\tif self.smooth != None:\n",
    "\t\t\tprob = self.smooth(word, undict=self.undict, total=self.total)\n",
    "\t\telse:\n",
    "\t\t\tif word in self.undict:\n",
    "\t\t\t\tprob = float(self.undict[word]) / self.total\n",
    "\t\treturn prob\n",
    "\n",
    "\tdef calc_sentence_prob(self, sentence, prob_log=True):\n",
    "\t\tprob_log_sum = 0\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tif word != const.START_TOKEN and word != const.END_TOKEN:\n",
    "\t\t\t\tword_prob = self.calc_prob(word)\n",
    "\t\t\t\tif word_prob != 0:\n",
    "\t\t\t\t\tprob_log_sum += math.log(word_prob, 2)\n",
    "\t\treturn math.pow(2, prob_log_sum) if prob_log else prob_log_sum\n",
    "\n",
    "\tdef sort_vocab(self):\n",
    "\t\tvocabs = list(self.undict.keys())\n",
    "\t\tvocabs.remove(const.START_TOKEN)\n",
    "\t\tvocabs.remove(const.END_TOKEN)\n",
    "\t\tvocabs.sort()\n",
    "\t\tvocabs.append(const.UNK)\n",
    "\t\tvocabs.append(const.START_TOKEN)\n",
    "\t\tvocabs.append(const.END_TOKEN)\n",
    "\t\treturn vocabs\n",
    "\n",
    "class BiGram(UnGram):\n",
    "\tdef __init__(self, sentences, smooth = None):\n",
    "\t\tUnGram.__init__(self, sentences, smooth)\n",
    "\t\tself.bidict = build_bidict(sentences)\n",
    "\n",
    "\tdef calc_prob(self, *args):\n",
    "\t\tif len(args) != 2:\n",
    "\t\t\traise ValueError('two words is required')\n",
    "\n",
    "\t\tprob = 0\n",
    "\t\tif self.smooth != None:\n",
    "\t\t\tprob = self.smooth(args[0], args[1], bidict=self.bidict, undict=self.undict)\n",
    "\t\telse:\n",
    "\t\t\tif args in self.bidict and args[0] in self.undict:\n",
    "\t\t\t\treturn float(self.bidict[args]) / self.undict[args[0]]\n",
    "\t\treturn prob\n",
    "\n",
    "\tdef calc_sentence_prob(self, sentence, prob_log=True):\n",
    "\t\tprob_log_sum = 0\n",
    "\t\tprev_word = None\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tif prev_word != None:\n",
    "\t\t\t\tword_prob = self.calc_prob(prev_word, word)\n",
    "\t\t\t\tprob_log_sum += word_prob\n",
    "\t\t\tprev_word = word\n",
    "\t\treturn math.pow(2, prob_log_sum) if prob_log else prob_log_sum\n",
    "\n",
    "\n",
    "class TriGram(BiGram):\n",
    "\tdef __init__(self, sentences, smooth = None):\n",
    "\t\tBiGram.__init__(self, sentences, smooth)\n",
    "\t\tself.tridict = build_tridict(sentences)\n",
    "\n",
    "\tdef calc_prob(self, *args):\n",
    "\t\tif len(args) != 3:\n",
    "\t\t\traise ValueError('three words is required')\n",
    "\n",
    "\t\tprob = 0\n",
    "\t\tif self.smooth != None:\n",
    "\t\t\tprob = self.smooth(args[0], args[1], args[2], tridict=self.tridict, bidict=self.bidict, undict=self.undict)\n",
    "\t\telse:\n",
    "\t\t\tbitup = (args[0], args[1])\t\t\t\t\n",
    "\t\t\tif args in self.tridict and bitup in self.bidict:\n",
    "\t\t\t\treturn float(self.tridict[args]) / self.bidict[bitup]\n",
    "\t\treturn prob\n",
    "\n",
    "\tdef calc_sentence_prob(self, sentence, prob_log=True):\n",
    "\t\tprob_log_sum = 0\n",
    "\t\tprev_stack = []\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tif len(prev_stack) < 2:\n",
    "\t\t\t\tprev_stack.append(word)\n",
    "\t\t\telif len(prev_stack) == 2:\n",
    "\t\t\t\tword_prob = self.calc_prob(prev_stack[0], prev_stack[1], word)\n",
    "\t\t\t\tprob_log_sum += word_prob\n",
    "\t\t\t\tprev_stack[0] = prev_stack[1]\n",
    "\t\t\t\tprev_stack[1] = word\n",
    "\t\treturn math.pow(2, prob_log_sum) if prob_log else prob_log_sum\n",
    "\n",
    "'''\n",
    "@function: calc_xxgram_count   主要用来统计语料库中词的总数\n",
    "@function: print_xxgram_probas 格式化输出概率 \n",
    "'''\n",
    "class GramUtil(object):\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef calc_ungram_count(sentences):\n",
    "\t\tcount = 0\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\t# except START_TOKEN and END_TOKEN\n",
    "\t\t\tcount += len(sentence) - 2\n",
    "\t\treturn count\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef calc_bigram_count(sentences):\n",
    "\t\tcount = 0\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\tcount += len(sentence) - 1\n",
    "\t\treturn count\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef calc_trigram_count(sentences):\n",
    "\t\tcount = 0\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\tcount += len(sentence)\n",
    "\t\treturn count\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef print_ungram_probs(model, vocabs):\n",
    "\t\tfor vocab in vocabs:\n",
    "\t\t\tif vocab != const.START_TOKEN and vocab != const.END_TOKEN:\n",
    "\t\t\t\tprint(\"{} \\t {}\".format(vocab if vocab != const.UNK else 'UNK', model.calc_prob(vocab)))\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef print_bigram_probs(model, vocabs):\n",
    "\t\tprint(\"\\t\\t\", end=\"\")\n",
    "\t\tfor vocab in vocabs:\n",
    "\t\t\tif vocab != const.START_TOKEN:\n",
    "\t\t\t\tprint(vocab if vocab != const.UNK else \"UNK\", end=\"\\t\\t\")\n",
    "\t\tprint(\"\")\n",
    "\t\tfor vocab in vocabs:\n",
    "\t\t\tif vocab != const.END_TOKEN:\n",
    "\t\t\t\tprint(vocab if vocab != const.UNK else \"UNK\", end=\"\\t\\t\")\n",
    "\t\t\t\tfor vocab2 in vocabs:\n",
    "\t\t\t\t\tif vocab2 != const.START_TOKEN:\n",
    "\t\t\t\t\t\tprint(\"{0:.3f}\".format(model.calc_prob(vocab, vocab2)), end=\"\\t\\t\")\n",
    "\t\t\t\tprint(\"\")\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef print_trigram_probs(model, vocabs):\n",
    "\t\tprint(\"\\t\\t\", end=\"\")\n",
    "\t\tfor vocab in vocabs:\n",
    "\t\t\tif vocab != const.START_TOKEN:\n",
    "\t\t\t\tprint(vocab if vocab != const.UNK else \"UNK\", end=\"\\t\")\n",
    "\t\tprint(\"\")\n",
    "\t\tfor vocab in vocabs:\n",
    "\t\t\tif vocab != const.END_TOKEN:\n",
    "\t\t\t\tfor vocab2 in vocabs:\n",
    "\t\t\t\t\tif vocab2 != const.START_TOKEN and vocab != const.UNK and vocab2 != const.UNK and vocab2 != const.END_TOKEN:\n",
    "\t\t\t\t\t\tprint(vocab, vocab2 if vocab2 != const.UNK else \"UNK\", end=\"\\t\\t\")\n",
    "\t\t\t\t\t\tfor vocab3 in vocabs:\n",
    "\t\t\t\t\t\t\tif vocab3 != const.END_TOKEN\n",
    "\t\t\t\t\t\t\t\tprint(\"{0:.3f}\".format(model.calc_prob(vocab, vocab2, vocab3)), end=\"\\t\")\n",
    "\t\t\t\t\t\tprint(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate.py\n",
    "\n",
    "模型的评估，这里主要用了困惑度Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# 计算困惑度\n",
    "def perplexity(model, sentences, cal_gram_func):\n",
    "    # gram_count 词的总数，对应教程中的 M\n",
    "\tgram_count = cal_gram_func(sentences)\n",
    "\tprob_log_sum = 0\n",
    "\tfor sentence in sentences:\n",
    "\t\ttry:\n",
    "\t\t\tprob_log_sum -= math.log(model.calc_sentence_prob(sentence), 2)\n",
    "\t\texcept:\n",
    "\t\t\tprob_log_sum -= float('-inf')\n",
    "\t\treturn math.pow(2, prob_log_sum/gram_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">  \n",
    "    <tr>  \n",
    "        <td>**#**</td>  \n",
    "        <td>**smooth**</td>\n",
    "        <td>**unsmooth**</td>\n",
    "    </tr>  \n",
    "    <tr>  \n",
    "        <td>你好不</td>\n",
    "        <td>2.99167</td>  \n",
    "        <td>3.97368</td>  \n",
    "    </tr>  \n",
    "    <tr>  \n",
    "        <td>好不你</td>\n",
    "        <td>1.10409</td>  \n",
    "        <td>1.21901</td>  \n",
    "    </tr>  \n",
    "    <tr>  \n",
    "        <td>你是不</td>\n",
    "        <td>1.75263</td>  \n",
    "        <td>2.06712</td>  \n",
    "    </tr>  \n",
    "</table>  \n",
    "<table class=\"table table-bordered\">  \n",
    "    <tr>  \n",
    "        <td>**#**</td>  \n",
    "        <td>**smooth**</td>\n",
    "        <td>**unsmooth**</td>\n",
    "    </tr>  \n",
    "    <tr>  \n",
    "        <td>Perplexity</td>\n",
    "        <td>0.91272</td>  \n",
    "        <td>0.89138</td>  \n",
    "    </tr>  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 项目后续\n",
    "\n",
    "过段时间会加入深度学习在语言模型上的应用，如果你感兴趣，可以关注我的公众号，或者star, watch 本项目哦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 联系作者\n",
    "\n",
    "@author sean\n",
    "\n",
    "@qq  929325776\n",
    "\n",
    "有什么问题，可以联系我，一起讨论"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
